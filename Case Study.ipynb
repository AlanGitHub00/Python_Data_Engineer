{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract data from tables stores in MariaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MariaDB database ...\n"
     ]
    }
   ],
   "source": [
    "# Import mysql.connect module \n",
    "# Using Python to extract data from tables stored in MariaDB\n",
    "import mysql.connector as mariadb\n",
    "mariadb = mariadb.connect(user = \"root\", password = \"\", database = \"cdw_sapp\")\n",
    "print('Connected to MariaDB database ...')\n",
    "cur = mariadb.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MariaDB → Spark SQL → MongoDB \n",
    "(1) Using Python to extract data from tables stored in MariaDB. \n",
    "(2) Transforming the data extracted from MariaDB using Spark (SparkSQL). \n",
    "(3) Loading the transformed data using Spark into MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------------------+-----------------+------------+----------+--------------+-------------------+\n",
      "|BRANCH_CODE| BRANCH_NAME|      BRANCH_STREET|      BRANCH_CITY|BRANCH_STATE|BRANCH_ZIP|  BRANCH_PHONE|       LAST_UPDATED|\n",
      "+-----------+------------+-------------------+-----------------+------------+----------+--------------+-------------------+\n",
      "|          1|Example Bank|       Bridle Court|        Lakeville|          MN|     55044|(123) 456-5276|2018-04-18 16:51:47|\n",
      "|          2|Example Bank|  Washington Street|          Huntley|          IL|     60142|(123) 461-8993|2018-04-18 16:51:47|\n",
      "|          3|Example Bank|      Warren Street|SouthRichmondHill|          NY|     11419|(123) 498-5926|2018-04-18 16:51:47|\n",
      "|          4|Example Bank|   Cleveland Street|       Middleburg|          FL|     32068|(123) 466-3064|2018-04-18 16:51:47|\n",
      "|          5|Example Bank|        14th Street|    KingOfPrussia|          PA|     19406|(123) 484-9701|2018-04-18 16:51:47|\n",
      "|          7|Example Bank|   Jefferson Street|         Paterson|          NJ|      7501|(123) 414-4890|2018-04-18 16:51:47|\n",
      "|          8|Example Bank|           B Street|        Pittsford|          NY|     14534|(123) 467-8272|2018-04-18 16:51:47|\n",
      "|          9|Example Bank|    Jefferson Court|     Wethersfield|          CT|      6109|(123) 467-5219|2018-04-18 16:51:47|\n",
      "|         10|Example Bank|     Cambridge Road|     NorthOlmsted|          OH|     44070|(123) 414-5047|2018-04-18 16:51:47|\n",
      "|         11|Example Bank|    3rd Street West|     Hillsborough|          NJ|      8844|(123) 436-6354|2018-04-18 16:51:47|\n",
      "|         12|Example Bank|          Mill Road|   MadisonHeights|          MI|     48071|(123) 486-7175|2018-04-18 16:51:47|\n",
      "|         14|Example Bank|  Washington Street|           Oviedo|          FL|     32765|(123) 493-8460|2018-04-18 16:51:47|\n",
      "|         15|Example Bank|    Chestnut Street|    Mechanicsburg|          PA|     17050|(123) 446-2043|2018-04-18 16:51:47|\n",
      "|         16|Example Bank|       Monroe Drive|        Plainview|          NY|     11803|(123) 485-7525|2018-04-18 16:51:47|\n",
      "|         17|Example Bank|    Railroad Street|          Paducah|          KY|     42001|(123) 454-6360|2018-04-18 16:51:47|\n",
      "|         18|Example Bank|Church Street South|     Harleysville|          PA|     19438|(123) 482-4455|2018-04-18 16:51:47|\n",
      "|         19|Example Bank|        King Street|     SilverSpring|          MD|     20901|(123) 448-4380|2018-04-18 16:51:47|\n",
      "|         20|Example Bank|   Canterbury Drive|       Burnsville|          MN|     55337|(123) 484-0138|2018-04-18 16:51:47|\n",
      "|         21|Example Bank|         2nd Avenue|           Tacoma|          WA|     98444|(123) 436-2433|2018-04-18 16:51:47|\n",
      "|         22|Example Bank| Front Street South|         Carlisle|          PA|     17013|(123) 492-2492|2018-04-18 16:51:47|\n",
      "+-----------+------------+-------------------+-----------------+------------+----------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pyspark.sql module\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# select the database from mariadb into spark \n",
    "# creating the dataframe using spark and the table name is cdw_sapp_creditcard\n",
    "df1 = spark.read.format(\"jdbc\").options(\n",
    "    url = \"jdbc:mysql://localhost:3306/cdw_sapp\",\n",
    "    driver = \"com.mysql.cj.jdbc.Driver\",\n",
    "    dbtable = \"cdw_sapp_branch\",\n",
    "    user = \"root\",\n",
    "    password=\"\").load()\n",
    "\n",
    "# creating the temp view for PySpark SQL and named table name called credit card\n",
    "df1.createOrReplaceTempView('branch')\n",
    "\n",
    "# Using spark.sql command to do transformation jobs \n",
    "# Note: df2 is the variable for credit card\n",
    "\n",
    "# Using spark.sql command to do transformation jobs \n",
    "# Note: df1 is the variable for branch\n",
    "df1 = spark.sql(\"SELECT BRANCH_CODE, BRANCH_NAME, BRANCH_STREET, BRANCH_CITY, BRANCH_STATE, IFNULL(branch_zip, 99999) BRANCH_ZIP, CONCAT('(',SUBSTR(branch_phone,1,3),') ',SUBSTR(branch_phone,4,3),'-', SUBSTR(branch_phone,7)) AS BRANCH_PHONE, LAST_UPDATED FROM branch\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------------------+-----------------+------------+----------+--------------+-------------------+\n",
      "|BRANCH_CODE| BRANCH_NAME|      BRANCH_STREET|      BRANCH_CITY|BRANCH_STATE|BRANCH_ZIP|  BRANCH_PHONE|       LAST_UPDATED|\n",
      "+-----------+------------+-------------------+-----------------+------------+----------+--------------+-------------------+\n",
      "|          1|Example Bank|       Bridle Court|        Lakeville|          MN|     55044|(123) 456-5276|2018-04-18 16:51:47|\n",
      "|          2|Example Bank|  Washington Street|          Huntley|          IL|     60142|(123) 461-8993|2018-04-18 16:51:47|\n",
      "|          3|Example Bank|      Warren Street|SouthRichmondHill|          NY|     11419|(123) 498-5926|2018-04-18 16:51:47|\n",
      "|          4|Example Bank|   Cleveland Street|       Middleburg|          FL|     32068|(123) 466-3064|2018-04-18 16:51:47|\n",
      "|          5|Example Bank|        14th Street|    KingOfPrussia|          PA|     19406|(123) 484-9701|2018-04-18 16:51:47|\n",
      "|          7|Example Bank|   Jefferson Street|         Paterson|          NJ|      7501|(123) 414-4890|2018-04-18 16:51:47|\n",
      "|          8|Example Bank|           B Street|        Pittsford|          NY|     14534|(123) 467-8272|2018-04-18 16:51:47|\n",
      "|          9|Example Bank|    Jefferson Court|     Wethersfield|          CT|      6109|(123) 467-5219|2018-04-18 16:51:47|\n",
      "|         10|Example Bank|     Cambridge Road|     NorthOlmsted|          OH|     44070|(123) 414-5047|2018-04-18 16:51:47|\n",
      "|         11|Example Bank|    3rd Street West|     Hillsborough|          NJ|      8844|(123) 436-6354|2018-04-18 16:51:47|\n",
      "|         12|Example Bank|          Mill Road|   MadisonHeights|          MI|     48071|(123) 486-7175|2018-04-18 16:51:47|\n",
      "|         14|Example Bank|  Washington Street|           Oviedo|          FL|     32765|(123) 493-8460|2018-04-18 16:51:47|\n",
      "|         15|Example Bank|    Chestnut Street|    Mechanicsburg|          PA|     17050|(123) 446-2043|2018-04-18 16:51:47|\n",
      "|         16|Example Bank|       Monroe Drive|        Plainview|          NY|     11803|(123) 485-7525|2018-04-18 16:51:47|\n",
      "|         17|Example Bank|    Railroad Street|          Paducah|          KY|     42001|(123) 454-6360|2018-04-18 16:51:47|\n",
      "|         18|Example Bank|Church Street South|     Harleysville|          PA|     19438|(123) 482-4455|2018-04-18 16:51:47|\n",
      "|         19|Example Bank|        King Street|     SilverSpring|          MD|     20901|(123) 448-4380|2018-04-18 16:51:47|\n",
      "|         20|Example Bank|   Canterbury Drive|       Burnsville|          MN|     55337|(123) 484-0138|2018-04-18 16:51:47|\n",
      "|         21|Example Bank|         2nd Avenue|           Tacoma|          WA|     98444|(123) 436-2433|2018-04-18 16:51:47|\n",
      "|         22|Example Bank| Front Street South|         Carlisle|          PA|     17013|(123) 492-2492|2018-04-18 16:51:47|\n",
      "+-----------+------------+-------------------+-----------------+------------+----------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connecting to MongoDB and the collection name called cdw_sapp_branch database name called cdw_sapp\n",
    "uri = \"mongodb://127.0.0.1/cdw_sapp.dbs\"\n",
    "spark_mongodb = SparkSession.builder.config(\"spark.mongodb.input.uri\",uri).config(\"spark.mongodb.output.uri\",uri).getOrCreate()\n",
    "df1.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode('append').option('database','cdw_sapp').option('collection','cdw_sapp_branch').save()\n",
    "df1.show()\n",
    "# The result should be 115 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. Credit Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+---------+-----------+----------------+-----------------+--------------+\n",
      "|      CUST_CC_NO|  TIMEID| CUST_SSN|BRANCH_CODE|TRANSACTION_TYPE|TRANSACTION_VALUE|TRANSACTION_ID|\n",
      "+----------------+--------+---------+-----------+----------------+-----------------+--------------+\n",
      "|4210653349028689|20180214|123459988|        114|       Education|           78.900|             1|\n",
      "|4210653349028689|20180320|123459988|         35|   Entertainment|           14.240|             2|\n",
      "|4210653349028689|20180708|123459988|        160|         Grocery|           56.700|             3|\n",
      "|4210653349028689|20180419|123459988|        114|   Entertainment|           59.730|             4|\n",
      "|4210653349028689|20181010|123459988|         93|             Gas|            3.590|             5|\n",
      "|4210653349028689|20180528|123459988|        164|       Education|            6.890|             6|\n",
      "|4210653349028689|20180519|123459988|        119|   Entertainment|           43.390|             7|\n",
      "|4210653349028689|20180808|123459988|         23|             Gas|           95.390|             8|\n",
      "|4210653349028689|20180318|123459988|        166|   Entertainment|           93.260|             9|\n",
      "|4210653349028689|20180903|123459988|         83|           Bills|          100.380|            10|\n",
      "|4210653349028689|20180821|123459988|         52|             Gas|           98.750|            11|\n",
      "|4210653349028689|20181224|123459988|         17|             Gas|           42.710|            12|\n",
      "|4210653349028689|20180403|123459988|         80|         Grocery|           40.240|            13|\n",
      "|4210653349028689|20180415|123459988|         50|           Bills|           17.810|            14|\n",
      "|4210653349028689|20180517|123459988|        123|           Bills|           29.000|            15|\n",
      "|4210653349028689|20180706|123459988|          9|            Test|           70.630|            16|\n",
      "|4210653349028689|20180928|123459988|          3|            Test|           27.040|            17|\n",
      "|4210653349028689|20180704|123459988|        135|   Entertainment|           88.750|            18|\n",
      "|4210653349028689|20180424|123459988|        103|            Test|           77.020|            19|\n",
      "|4210653349028689|20181008|123459988|         78|           Bills|           34.340|            20|\n",
      "+----------------+--------+---------+-----------+----------------+-----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pyspark.sql module\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# select the database from mariadb into spark \n",
    "# creating the dataframe using spark and the table name is cdw_sapp_creditcard\n",
    "df2 = spark.read.format(\"jdbc\").options(\n",
    "    url = \"jdbc:mysql://localhost:3306/cdw_sapp\",\n",
    "    driver = \"com.mysql.cj.jdbc.Driver\",\n",
    "    dbtable = \"cdw_sapp_creditcard\",\n",
    "    user = \"root\",\n",
    "    password=\"\").load()\n",
    "\n",
    "# creating the temp view for PySpark SQL and named table name called credit card\n",
    "df2.createOrReplaceTempView('creditcard')\n",
    "\n",
    "# Using spark.sql command to do transformation jobs \n",
    "# Note: df2 is the variable for credit card\n",
    "df2 = spark.sql('SELECT CREDIT_CARD_NO CUST_CC_NO, CONCAT(YEAR, LPAD(Month, 2, 0), LPAD(Day, 2, 0)) TIMEID, CUST_SSN, BRANCH_CODE, TRANSACTION_TYPE, TRANSACTION_VALUE, TRANSACTION_ID FROM creditcard')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+---------+-----------+----------------+-----------------+--------------+\n",
      "|      CUST_CC_NO|  TIMEID| CUST_SSN|BRANCH_CODE|TRANSACTION_TYPE|TRANSACTION_VALUE|TRANSACTION_ID|\n",
      "+----------------+--------+---------+-----------+----------------+-----------------+--------------+\n",
      "|4210653349028689|20180214|123459988|        114|       Education|           78.900|             1|\n",
      "|4210653349028689|20180320|123459988|         35|   Entertainment|           14.240|             2|\n",
      "|4210653349028689|20180708|123459988|        160|         Grocery|           56.700|             3|\n",
      "|4210653349028689|20180419|123459988|        114|   Entertainment|           59.730|             4|\n",
      "|4210653349028689|20181010|123459988|         93|             Gas|            3.590|             5|\n",
      "|4210653349028689|20180528|123459988|        164|       Education|            6.890|             6|\n",
      "|4210653349028689|20180519|123459988|        119|   Entertainment|           43.390|             7|\n",
      "|4210653349028689|20180808|123459988|         23|             Gas|           95.390|             8|\n",
      "|4210653349028689|20180318|123459988|        166|   Entertainment|           93.260|             9|\n",
      "|4210653349028689|20180903|123459988|         83|           Bills|          100.380|            10|\n",
      "|4210653349028689|20180821|123459988|         52|             Gas|           98.750|            11|\n",
      "|4210653349028689|20181224|123459988|         17|             Gas|           42.710|            12|\n",
      "|4210653349028689|20180403|123459988|         80|         Grocery|           40.240|            13|\n",
      "|4210653349028689|20180415|123459988|         50|           Bills|           17.810|            14|\n",
      "|4210653349028689|20180517|123459988|        123|           Bills|           29.000|            15|\n",
      "|4210653349028689|20180706|123459988|          9|            Test|           70.630|            16|\n",
      "|4210653349028689|20180928|123459988|          3|            Test|           27.040|            17|\n",
      "|4210653349028689|20180704|123459988|        135|   Entertainment|           88.750|            18|\n",
      "|4210653349028689|20180424|123459988|        103|            Test|           77.020|            19|\n",
      "|4210653349028689|20181008|123459988|         78|           Bills|           34.340|            20|\n",
      "+----------------+--------+---------+-----------+----------------+-----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connecting to MongoDB and the collection name called cdw_sapp_creditcard database name called cdw_sapp\n",
    "uri = \"mongodb://127.0.0.1/cdw_sapp.dbs\"\n",
    "spark_mongodb = SparkSession.builder.config(\"spark.mongodb.input.uri\",uri).config(\"spark.mongodb.output.uri\",uri).getOrCreate()\n",
    "df2.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode('append').option('database','cdw_sapp').option('collection','cdw_sapp_creditcard').save()\n",
    "df2.show()\n",
    "# The result should be 46694 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+-----------+----------------+--------------------+------------+----------+-------------+--------+----------+--------------------+-------------------+\n",
      "| CUST_SSN|CUST_F_NAME|CUST_M_NAME|CUST_L_NAME|      CUST_CC_NO|         CUST_STREET|   CUST_CITY|CUST_STATE| CUST_COUNTRY|CUST_ZIP|CUST_PHONE|          CUST_EMAIL|       LAST_UPDATED|\n",
      "+---------+-----------+-----------+-----------+----------------+--------------------+------------+----------+-------------+--------+----------+--------------------+-------------------+\n",
      "|123456100|       Alec|         wm|     Hooper|4210653310061055|656 Main Street N...|     Natchez|        MS|United States|   39120| 12-37-818| AHooper@example.com|2018-04-21 12:49:02|\n",
      "|123453023|       Etta|    brendan|     Holman|4210653310102868|   829 Redwood Drive|Wethersfield|        CT|United States|   06109| 12-38-933| EHolman@example.com|2018-04-21 12:49:02|\n",
      "|123454487|     Wilber|   ezequiel|     Dunham|4210653310116272|683 12th Street East|     Huntley|        IL|United States|   60142| 12-43-018| WDunham@example.com|2018-04-21 12:49:02|\n",
      "|123459758|    Eugenio|      trina|      Hardy|4210653310195948|253 Country Club ...|   NewBerlin|        WI|United States|   53151| 12-43-215|  EHardy@example.com|2018-04-21 12:49:02|\n",
      "|123454431|    Wilfred|        may|      Ayers|4210653310356919|  301 Madison Street|      ElPaso|        TX|United States|   79930| 12-42-074|  WAyers@example.com|2018-04-21 12:49:02|\n",
      "|123454202|       Beau|    ambrose|    Woodard|4210653310395982|    3 Colonial Drive|NorthOlmsted|        OH|United States|   44070| 12-42-570|BWoodard@example.com|2018-04-21 12:49:02|\n",
      "|123451799|     Sheila|      larry|       Kemp|4210653310400536|   84 Belmont Avenue|      Vienna|        VA|United States|   22180| 12-39-685|   SKemp@example.com|2018-04-21 12:49:02|\n",
      "|123453875|      Wendy|        ora|     Hurley|4210653310459911|    728 Oxford Court|      Duarte|        CA|United States|   91010| 12-38-213| WHurley@example.com|2018-04-21 12:49:02|\n",
      "|123457511|       Alec|     tracie|    Gilmore|4210653310773972|    81 Forest Street|      Owosso|        MI|United States|   48867| 12-40-689|AGilmore@example.com|2018-04-21 12:49:02|\n",
      "|123457464|     Barbra|    mitchel|        Lau|4210653310794854|    561 Court Street|        Zion|        IL|United States|   60099| 12-35-222|    BLau@example.com|2018-04-21 12:49:02|\n",
      "|123457639|    Edmundo|      denny|    Thomson|4210653310817373|   622 Cypress Court|  Youngstown|        OH|United States|   44512| 12-41-363|EThomson@example.com|2018-04-21 12:49:02|\n",
      "|123453242|       Elsa|   isabelle|     Truong|4210653310844617| 924 8th Street West| Summerville|        SC|United States|   29483| 12-36-228| ETruong@example.com|2018-04-21 12:49:02|\n",
      "|123454339|      Homer|      henry|   Mckinney|4210653311015303|     611 East Avenue|      ElPaso|        TX|United States|   79930| 12-38-165|HMckinney@example...|2018-04-21 12:49:02|\n",
      "|123454537|       Rita|     rickey|       Kidd|4210653311215039|        680 Route 44|      Fenton|        MI|United States|   48430| 12-34-730|   RKidd@example.com|2018-04-21 12:49:02|\n",
      "|123452373|     Amalia|  heriberto|    Ballard|4210653311229354|    71 Warren Street|  Grandville|        MI|United States|   49418| 12-42-113|ABallard@example.com|2018-04-21 12:49:02|\n",
      "|123455343|      Patty|   angelita|     Thomas|4210653311652836|    195 Jones Street|    YubaCity|        CA|United States|   95993| 12-39-888| PThomas@example.com|2018-04-21 12:49:02|\n",
      "|123451533|   Josefina|   dorothea|     Morrow|4210653311707126|      500 New Street|   CapeCoral|        FL|United States|   33904| 12-40-158| JMorrow@example.com|2018-04-21 12:49:02|\n",
      "|123459278|     Nelson|  jefferson|    Andrews|4210653311730764| 989 Division Street|  Brookfield|        WI|United States|   53045| 12-41-408|NAndrews@example.com|2018-04-21 12:49:02|\n",
      "|123456915|     Miquel|     maximo|  Schneider|4210653311898082|    810 Maple Street|    Richmond|        VA|United States|   23223| 12-38-390|MSchneider@exampl...|2018-04-21 12:49:02|\n",
      "|123453807|     Parker|    arnulfo|    Tidwell|4210653312021765|      649 Eagle Road| WestChester|        PA|United States|   19380| 12-35-067|PTidwell@example.com|2018-04-21 12:49:02|\n",
      "+---------+-----------+-----------+-----------+----------------+--------------------+------------+----------+-------------+--------+----------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pyspark.sql module\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# select the database from mariadb into spark \n",
    "# creating the dataframe using spark and the table name is cdw_sapp_customer\n",
    "df3 = spark.read.format(\"jdbc\").options(\n",
    "    url = \"jdbc:mysql://localhost:3306/cdw_sapp\",\n",
    "    driver = \"com.mysql.cj.jdbc.Driver\",\n",
    "    dbtable = \"cdw_sapp_customer\",\n",
    "    user = \"root\",\n",
    "    password=\"\").load()\n",
    "\n",
    "# creating the temp view for PySpark SQL and named table name called credit card\n",
    "df3.createOrReplaceTempView('customer')\n",
    "\n",
    "# Using spark.sql command to do transformation jobs \n",
    "# Note: df3 is the variable for customer\n",
    "df3 = spark.sql(\"SELECT SSN CUST_SSN, \\\n",
    "       CONCAT(UCASE(SUBSTRING(`FIRST_NAME`, 1, 1)), LOWER(SUBSTRING(`FIRST_NAME`, 2))) AS CUST_F_NAME, \\\n",
    "       CONCAT(LCASE(SUBSTRING(`MIDDLE_NAME`, 1, 1)), LOWER(SUBSTRING(`MIDDLE_NAME`, 2))) AS CUST_M_NAME, \\\n",
    "       CONCAT(UCASE(SUBSTRING(`LAST_NAME`, 1, 1)), LOWER(SUBSTRING(`LAST_NAME`, 2))) AS CUST_L_NAME, \\\n",
    "       CREDIT_CARD_NO CUST_CC_NO, CONCAT(APT_NO, ' ', STREET_NAME) AS CUST_STREET, \\\n",
    "       CUST_CITY, \\\n",
    "       CUST_STATE, \\\n",
    "       CUST_COUNTRY, \\\n",
    "       CUST_ZIP, \\\n",
    "       CONCAT(SUBSTRING(cust_phone,1,2), '-', SUBSTRING(cust_phone,3,2), '-', SUBSTRING(cust_phone,5,7)) AS CUST_PHONE, \\\n",
    "       CUST_EMAIL, \\\n",
    "       LAST_UPDATED \\\n",
    "       FROM customer\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+-----------+----------------+--------------------+------------+----------+-------------+--------+----------+--------------------+-------------------+\n",
      "| CUST_SSN|CUST_F_NAME|CUST_M_NAME|CUST_L_NAME|      CUST_CC_NO|         CUST_STREET|   CUST_CITY|CUST_STATE| CUST_COUNTRY|CUST_ZIP|CUST_PHONE|          CUST_EMAIL|       LAST_UPDATED|\n",
      "+---------+-----------+-----------+-----------+----------------+--------------------+------------+----------+-------------+--------+----------+--------------------+-------------------+\n",
      "|123456100|       Alec|         wm|     Hooper|4210653310061055|656 Main Street N...|     Natchez|        MS|United States|   39120| 12-37-818| AHooper@example.com|2018-04-21 12:49:02|\n",
      "|123453023|       Etta|    brendan|     Holman|4210653310102868|   829 Redwood Drive|Wethersfield|        CT|United States|   06109| 12-38-933| EHolman@example.com|2018-04-21 12:49:02|\n",
      "|123454487|     Wilber|   ezequiel|     Dunham|4210653310116272|683 12th Street East|     Huntley|        IL|United States|   60142| 12-43-018| WDunham@example.com|2018-04-21 12:49:02|\n",
      "|123459758|    Eugenio|      trina|      Hardy|4210653310195948|253 Country Club ...|   NewBerlin|        WI|United States|   53151| 12-43-215|  EHardy@example.com|2018-04-21 12:49:02|\n",
      "|123454431|    Wilfred|        may|      Ayers|4210653310356919|  301 Madison Street|      ElPaso|        TX|United States|   79930| 12-42-074|  WAyers@example.com|2018-04-21 12:49:02|\n",
      "|123454202|       Beau|    ambrose|    Woodard|4210653310395982|    3 Colonial Drive|NorthOlmsted|        OH|United States|   44070| 12-42-570|BWoodard@example.com|2018-04-21 12:49:02|\n",
      "|123451799|     Sheila|      larry|       Kemp|4210653310400536|   84 Belmont Avenue|      Vienna|        VA|United States|   22180| 12-39-685|   SKemp@example.com|2018-04-21 12:49:02|\n",
      "|123453875|      Wendy|        ora|     Hurley|4210653310459911|    728 Oxford Court|      Duarte|        CA|United States|   91010| 12-38-213| WHurley@example.com|2018-04-21 12:49:02|\n",
      "|123457511|       Alec|     tracie|    Gilmore|4210653310773972|    81 Forest Street|      Owosso|        MI|United States|   48867| 12-40-689|AGilmore@example.com|2018-04-21 12:49:02|\n",
      "|123457464|     Barbra|    mitchel|        Lau|4210653310794854|    561 Court Street|        Zion|        IL|United States|   60099| 12-35-222|    BLau@example.com|2018-04-21 12:49:02|\n",
      "|123457639|    Edmundo|      denny|    Thomson|4210653310817373|   622 Cypress Court|  Youngstown|        OH|United States|   44512| 12-41-363|EThomson@example.com|2018-04-21 12:49:02|\n",
      "|123453242|       Elsa|   isabelle|     Truong|4210653310844617| 924 8th Street West| Summerville|        SC|United States|   29483| 12-36-228| ETruong@example.com|2018-04-21 12:49:02|\n",
      "|123454339|      Homer|      henry|   Mckinney|4210653311015303|     611 East Avenue|      ElPaso|        TX|United States|   79930| 12-38-165|HMckinney@example...|2018-04-21 12:49:02|\n",
      "|123454537|       Rita|     rickey|       Kidd|4210653311215039|        680 Route 44|      Fenton|        MI|United States|   48430| 12-34-730|   RKidd@example.com|2018-04-21 12:49:02|\n",
      "|123452373|     Amalia|  heriberto|    Ballard|4210653311229354|    71 Warren Street|  Grandville|        MI|United States|   49418| 12-42-113|ABallard@example.com|2018-04-21 12:49:02|\n",
      "|123455343|      Patty|   angelita|     Thomas|4210653311652836|    195 Jones Street|    YubaCity|        CA|United States|   95993| 12-39-888| PThomas@example.com|2018-04-21 12:49:02|\n",
      "|123451533|   Josefina|   dorothea|     Morrow|4210653311707126|      500 New Street|   CapeCoral|        FL|United States|   33904| 12-40-158| JMorrow@example.com|2018-04-21 12:49:02|\n",
      "|123459278|     Nelson|  jefferson|    Andrews|4210653311730764| 989 Division Street|  Brookfield|        WI|United States|   53045| 12-41-408|NAndrews@example.com|2018-04-21 12:49:02|\n",
      "|123456915|     Miquel|     maximo|  Schneider|4210653311898082|    810 Maple Street|    Richmond|        VA|United States|   23223| 12-38-390|MSchneider@exampl...|2018-04-21 12:49:02|\n",
      "|123453807|     Parker|    arnulfo|    Tidwell|4210653312021765|      649 Eagle Road| WestChester|        PA|United States|   19380| 12-35-067|PTidwell@example.com|2018-04-21 12:49:02|\n",
      "+---------+-----------+-----------+-----------+----------------+--------------------+------------+----------+-------------+--------+----------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connecting to MongoDB and the collection name called cdw_sapp_customer database name called cdw_sapp\n",
    "uri = \"mongodb://127.0.0.1/cdw_sapp.dbs\"\n",
    "spark_mongodb = SparkSession.builder.config(\"spark.mongodb.input.uri\",uri).config(\"spark.mongodb.output.uri\",uri).getOrCreate()\n",
    "df3.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode('append').option('database','cdw_sapp').option('collection','cdw_sapp_customer').save()\n",
    "df3.show()\n",
    "# The result should be 952 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. URL or Document to Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Extracting JSON, CSV and other types of data from web URLs using the Spark Streaming integration for Kafka, a stream-processing software. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)  Using the Kafka connector in the Spark Structured Streaming in order to manage the transformation of the data before loading into MongoDB collections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) And Sending it to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. The Database Name in this code is called health_insurance and collection name is health_insurance_insurancecharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get the data from the following url: \n",
    "# https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/insurance.txt\n",
    "import requests, os\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "def kafka_prod():\n",
    "    producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "    response = requests.get(\"https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/insurance.txt\")\n",
    " \n",
    "    data_list = [data for data in response.text.splitlines()[1:]]\n",
    "    for data in data_list:\n",
    "        #print(data)\n",
    "        producer.send('insurance', data.encode('utf-8'))\n",
    "    producer.flush()\n",
    "\n",
    "\n",
    "def spark_kafka():\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1 pyspark-shell'\n",
    "    # conf=SparkConf()\n",
    "    # conf.set(\"spark.executor.memory\", \"4g\")\n",
    "    # conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    raw_kafka_df = spark.readStream \\\n",
    "                        .format(\"kafka\") \\\n",
    "                        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "                        .option(\"subscribe\", 'insurance') \\\n",
    "                        .option(\"startingOffsets\", \"earliest\") \\\n",
    "                        .load()\n",
    "\n",
    "    kafka_value_df = raw_kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "    output_query = kafka_value_df.writeStream \\\n",
    "                          .queryName(\"insurance\") \\\n",
    "                          .format(\"memory\") \\\n",
    "                          .start()\n",
    "    output_query.awaitTermination(10)\n",
    "\n",
    "    value_df = spark.sql(\"select * from insurance\")\n",
    "    value_df.show()\n",
    "    \n",
    "    value_rdd = value_df.rdd.map(lambda i: i['value'].split(\"\\t\"))\n",
    "    value_row_rdd = value_rdd.map(lambda i: Row(age=i[0], \\\n",
    "                                                sex=i[1], \\\n",
    "                                                bmi=float(i[2]), \\\n",
    "                                                children=int(i[3]), \\\n",
    "                                                smoker=i[4], \\\n",
    "                                                region=i[5], \\\n",
    "                                             charges=i[6]))\n",
    "\n",
    "    df = spark.createDataFrame(value_row_rdd)\n",
    "    df.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # df.printSchema()\n",
    "    \n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "         .mode('append') \\\n",
    "         .option('database','health_insurance') \\\n",
    "         .option('collection', 'health_insurance_insurancecharges') \\\n",
    "         .option('uri', \"mongodb://127.0.0.1/health_insurance.dbs\") \\\n",
    "         .save()\n",
    "\n",
    "def main():\n",
    "    kafka_prod()\n",
    "    spark_kafka()\n",
    "#     \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. The Database Name in this code is called health_insurance and collection name is health_insurance_benefit, note: since the text file is very large, hence we break it in 4 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get the data from the following url: \n",
    "# https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/BenefitsCostSharing_partOne.txt\n",
    "import requests, os\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "def kafka_prod():\n",
    "    producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "    response = requests.get(\"https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/BenefitsCostSharing_partOne.txt\")\n",
    " \n",
    "    data_list = [data for data in response.text.splitlines()[1:]]\n",
    "    for data in data_list:\n",
    "        #print(data)\n",
    "        producer.send('benefit1', data.encode('utf-8'))\n",
    "    producer.flush()\n",
    "\n",
    "\n",
    "def spark_kafka():\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1 pyspark-shell'\n",
    "    # conf=SparkConf()\n",
    "    # conf.set(\"spark.executor.memory\", \"4g\")\n",
    "    # conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    raw_kafka_df = spark.readStream \\\n",
    "                        .format(\"kafka\") \\\n",
    "                        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "                        .option(\"subscribe\", 'benefit1') \\\n",
    "                        .option(\"startingOffsets\", \"earliest\") \\\n",
    "                        .load()\n",
    "\n",
    "    kafka_value_df = raw_kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "    output_query = kafka_value_df.writeStream \\\n",
    "                          .queryName(\"benefit1\") \\\n",
    "                          .format(\"memory\") \\\n",
    "                          .start()\n",
    "    output_query.awaitTermination(10)\n",
    "\n",
    "    value_df = spark.sql(\"select * from benefit1\")\n",
    "    value_df.show()\n",
    "    \n",
    "    value_rdd = value_df.rdd.map(lambda i: i['value'].split(\"\\t\"))\n",
    "    value_row_rdd = value_rdd.map(lambda i: Row(BenefitName=i[0], \\\n",
    "                                                BusinessYear=i[1], \\\n",
    "                                                EHBVarReason=i[2], \\\n",
    "                                                IsCovered=i[3], \\\n",
    "                                                IssuerId=i[4], \\\n",
    "                                                LimitQty=i[5], \\\n",
    "                                                LimitUnit=i[6], \\\n",
    "                                                MinimumStay=i[7], \\\n",
    "                                                PlanId=i[8], \\\n",
    "                                                SourceName=i[9], \\\n",
    "                                                StateCode=i[10]))\n",
    "\n",
    "    df = spark.createDataFrame(value_row_rdd)\n",
    "    df.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # df.printSchema()\n",
    "    \n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "         .mode('append') \\\n",
    "         .option('database','health_insurance') \\\n",
    "         .option('collection', 'health_insurance_benefit') \\\n",
    "         .option('uri', \"mongodb://127.0.0.1/health_insurance.dbs\") \\\n",
    "         .save()\n",
    "\n",
    "def main():\n",
    "    kafka_prod()\n",
    "    spark_kafka()\n",
    "#     \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get the data from the following url: \n",
    "# https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/BenefitsCostSharing_partTwo.txt\n",
    "import requests, os\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "def kafka_prod():\n",
    "    producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "    response = requests.get(\"https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/BenefitsCostSharing_partTwo.txt\")\n",
    " \n",
    "    data_list = [data for data in response.text.splitlines()[1:]]\n",
    "    for data in data_list:\n",
    "        #print(data)\n",
    "        producer.send('benefit2', data.encode('utf-8'))\n",
    "    producer.flush()\n",
    "\n",
    "\n",
    "def spark_kafka():\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1 pyspark-shell'\n",
    "    # conf=SparkConf()\n",
    "    # conf.set(\"spark.executor.memory\", \"4g\")\n",
    "    # conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    raw_kafka_df = spark.readStream \\\n",
    "                        .format(\"kafka\") \\\n",
    "                        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "                        .option(\"subscribe\", 'benefit2') \\\n",
    "                        .option(\"startingOffsets\", \"earliest\") \\\n",
    "                        .load()\n",
    "\n",
    "    kafka_value_df = raw_kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "    output_query = kafka_value_df.writeStream \\\n",
    "                          .queryName(\"benefit2\") \\\n",
    "                          .format(\"memory\") \\\n",
    "                          .start()\n",
    "    output_query.awaitTermination(10)\n",
    "\n",
    "    value_df = spark.sql(\"select * from benefit2\")\n",
    "    value_df.show()\n",
    "    \n",
    "    value_rdd = value_df.rdd.map(lambda i: i['value'].split(\"\\t\"))\n",
    "    value_row_rdd = value_rdd.map(lambda i: Row(BenefitName=i[0], \\\n",
    "                                                BusinessYear=i[1], \\\n",
    "                                                EHBVarReason=i[2], \\\n",
    "                                                IsCovered=i[3], \\\n",
    "                                                IssuerId=i[4], \\\n",
    "                                                LimitQty=i[5], \\\n",
    "                                                LimitUnit=i[6], \\\n",
    "                                                MinimumStay=i[7], \\\n",
    "                                                PlanId=i[8], \\\n",
    "                                                SourceName=i[9], \\\n",
    "                                                StateCode=i[10]))\n",
    "\n",
    "    df = spark.createDataFrame(value_row_rdd)\n",
    "    df.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # df.printSchema()\n",
    "    \n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "         .mode('append') \\\n",
    "         .option('database','health_insurance') \\\n",
    "         .option('collection', 'health_insurance_benefit') \\\n",
    "         .option('uri', \"mongodb://127.0.0.1/health_insurance.dbs\") \\\n",
    "         .save()\n",
    "\n",
    "def main():\n",
    "    kafka_prod()\n",
    "    spark_kafka()\n",
    "#     \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get the data from the following url: \n",
    "# https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/BenefitsCostSharing_partThree.txt\n",
    "import requests, os\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "def kafka_prod():\n",
    "    producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "    response = requests.get(\"https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/BenefitsCostSharing_partThree.txt\")\n",
    " \n",
    "    data_list = [data for data in response.text.splitlines()[1:]]\n",
    "    for data in data_list:\n",
    "        #print(data)\n",
    "        producer.send('benefit3', data.encode('utf-8'))\n",
    "    producer.flush()\n",
    "\n",
    "\n",
    "def spark_kafka():\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1 pyspark-shell'\n",
    "    # conf=SparkConf()\n",
    "    # conf.set(\"spark.executor.memory\", \"4g\")\n",
    "    # conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    raw_kafka_df = spark.readStream \\\n",
    "                        .format(\"kafka\") \\\n",
    "                        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "                        .option(\"subscribe\", 'benefit3') \\\n",
    "                        .option(\"startingOffsets\", \"earliest\") \\\n",
    "                        .load()\n",
    "\n",
    "    kafka_value_df = raw_kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "    output_query = kafka_value_df.writeStream \\\n",
    "                          .queryName(\"benefit3\") \\\n",
    "                          .format(\"memory\") \\\n",
    "                          .start()\n",
    "    output_query.awaitTermination(10)\n",
    "\n",
    "    value_df = spark.sql(\"select * from benefit3\")\n",
    "    value_df.show()\n",
    "    \n",
    "    value_rdd = value_df.rdd.map(lambda i: i['value'].split(\"\\t\"))\n",
    "    value_row_rdd = value_rdd.map(lambda i: Row(BenefitName=i[0], \\\n",
    "                                                BusinessYear=i[1], \\\n",
    "                                                EHBVarReason=i[2], \\\n",
    "                                                IsCovered=i[3], \\\n",
    "                                                IssuerId=i[4], \\\n",
    "                                                LimitQty=i[5], \\\n",
    "                                                LimitUnit=i[6], \\\n",
    "                                                MinimumStay=i[7], \\\n",
    "                                                PlanId=i[8], \\\n",
    "                                                SourceName=i[9], \\\n",
    "                                                StateCode=i[10]))\n",
    "\n",
    "    df = spark.createDataFrame(value_row_rdd)\n",
    "    df.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # df.printSchema()\n",
    "    \n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "         .mode('append') \\\n",
    "         .option('database','health_insurance') \\\n",
    "         .option('collection', 'health_insurance_benefit') \\\n",
    "         .option('uri', \"mongodb://127.0.0.1/health_insurance.dbs\") \\\n",
    "         .save()\n",
    "\n",
    "def main():\n",
    "    kafka_prod()\n",
    "    spark_kafka()\n",
    "#     \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get the data from the following url: \n",
    "# https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/BenefitsCostSharing_partFour.txt\n",
    "import requests, os\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "def kafka_prod():\n",
    "    producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "    response = requests.get(\"https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/BenefitsCostSharing_partFour.txt\")\n",
    " \n",
    "    data_list = [data for data in response.text.splitlines()[1:]]\n",
    "    for data in data_list:\n",
    "        #print(data)\n",
    "        producer.send('benefitcost4', data.encode('utf-8'))\n",
    "    producer.flush()\n",
    "\n",
    "\n",
    "def spark_kafka():\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1 pyspark-shell'\n",
    "    # conf=SparkConf()\n",
    "    # conf.set(\"spark.executor.memory\", \"4g\")\n",
    "    # conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    raw_kafka_df = spark.readStream \\\n",
    "                        .format(\"kafka\") \\\n",
    "                        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "                        .option(\"subscribe\", 'benefitcost4') \\\n",
    "                        .option(\"startingOffsets\", \"earliest\") \\\n",
    "                        .load()\n",
    "\n",
    "    kafka_value_df = raw_kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "    output_query = kafka_value_df.writeStream \\\n",
    "                          .queryName(\"benefitcost4\") \\\n",
    "                          .format(\"memory\") \\\n",
    "                          .start()\n",
    "    output_query.awaitTermination(10)\n",
    "\n",
    "    value_df = spark.sql(\"select * from benefitcost4\")\n",
    "    value_df.show()\n",
    "    \n",
    "    value_rdd = value_df.rdd.map(lambda i: i['value'].split(\"\\t\"))\n",
    "    value_row_rdd = value_rdd.map(lambda i: Row(BenefitName=i[0], \\\n",
    "                                                BusinessYear=i[1], \\\n",
    "                                                EHBVarReason=i[2], \\\n",
    "                                                IsCovered=i[3], \\\n",
    "                                                IssuerId=i[4], \\\n",
    "                                                LimitQty=i[5], \\\n",
    "                                                LimitUnit=i[6], \\\n",
    "                                                MinimumStay=i[7], \\\n",
    "                                                PlanId=i[8], \\\n",
    "                                                SourceName=i[9], \\\n",
    "                                                StateCode=i[10]))\n",
    "\n",
    "    df = spark.createDataFrame(value_row_rdd)\n",
    "    df.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # df.printSchema()\n",
    "    \n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "         .mode('append') \\\n",
    "         .option('database','health_insurance') \\\n",
    "         .option('collection', 'health_insurance_benefit') \\\n",
    "         .option('uri', \"mongodb://127.0.0.1/health_insurance.dbs\") \\\n",
    "         .save()\n",
    "\n",
    "def main():\n",
    "    kafka_prod()\n",
    "    spark_kafka()\n",
    "#     \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. The Database Name in this code is called health_insurance and collection name is health_insurance_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get the data from the following url: \n",
    "# https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/ServiceArea.csv\n",
    "import requests, os\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "def kafka_prod():\n",
    "    producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "    response = requests.get(\"https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/ServiceArea.csv\")\n",
    " \n",
    "    data_list = [data for data in response.text.splitlines()[1:]]\n",
    "    for data in data_list:\n",
    "        #print(data)\n",
    "        producer.send('service2', data.encode('utf-8'))\n",
    "    producer.flush()\n",
    "\n",
    "\n",
    "def spark_kafka():\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1 pyspark-shell'\n",
    "    # conf=SparkConf()\n",
    "    # conf.set(\"spark.executor.memory\", \"4g\")\n",
    "    # conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    spark = SparkSession.builder.config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/\").getOrCreate()\n",
    "    \n",
    "    raw_kafka_df = spark.readStream \\\n",
    "                        .format(\"kafka\") \\\n",
    "                        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "                        .option(\"subscribe\", 'service2') \\\n",
    "                        .option(\"startingOffsets\", \"earliest\") \\\n",
    "                        .load()\n",
    "\n",
    "    kafka_value_df = raw_kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "    output_query = kafka_value_df.writeStream \\\n",
    "                          .queryName(\"service2\") \\\n",
    "                          .format(\"memory\") \\\n",
    "                          .start()\n",
    "    output_query.awaitTermination(10)\n",
    "\n",
    "    value_df = spark.sql(\"select * from service2\")\n",
    "    value_df.show()\n",
    "    \n",
    "    value_rdd = value_df.rdd.map(lambda i: i['value'].split(\",\"))\n",
    "    value_row_rdd = value_rdd.map(lambda i: Row(BusinessYear=int(i[0]), \\\n",
    "                                                StateCode=i[1], \\\n",
    "                                                IssuerId=int(i[2]), \\\n",
    "                                                SourceName=i[3], \\\n",
    "                                                VersionNum=int(i[4]), \\\n",
    "                                                ImportDate=i[5], \\\n",
    "                                                IssuerId2=int(i[6]), \\\n",
    "                                                StateCode2=i[7], \\\n",
    "                                                ServiceAreaId =i[8], \\\n",
    "                                                ServiceAreaName = i[9], \\\n",
    "                                                CoverEntireState=i[10], \\\n",
    "                                                County=i[11], \\\n",
    "                                                PartialCounty=i[12], \\\n",
    "                                                ZipCodes=i[13], \\\n",
    "                                                PartialCountyJustification=i[14], \\\n",
    "                                                RowNumber=i[15], \\\n",
    "                                                MarketCoverage=i[16], \\\n",
    "                                                DentalOnlyPlan=i[17]))\n",
    "\n",
    "    df = spark.createDataFrame(value_row_rdd)\n",
    "    df.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # df.printSchema()\n",
    "    \n",
    "#     df.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "#         .mode('append') \\\n",
    "#         .option('database','cdw_sapp') \\\n",
    "#         .option('collection', 'cdw_sapp_service') \\\n",
    "#         .option('uri', \"mongodb://127.0.0.1/cdw_sapp.dbs\") \\\n",
    "#         .save()\n",
    "\n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .mode('append') \\\n",
    "        .option('database','health_insurance') \\\n",
    "        .option('collection', 'health_insurance_service') \\\n",
    "        .save()\n",
    "\n",
    "def main():\n",
    "    kafka_prod()\n",
    "    spark_kafka()\n",
    "#     \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4. The Database Name in this code is called health_insurance and collection name is health_insurance_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get the data from the following url: \n",
    "# https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/PlanAttributes.csv\n",
    "import requests, os\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "def kafka_prod():\n",
    "    producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "    response = requests.get(\"https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/PlanAttributes.csv\")\n",
    " \n",
    "    data_list = [data for data in response.text.splitlines()[1:]]\n",
    "    for data in data_list:\n",
    "        #print(data)\n",
    "        producer.send('plans', data.encode('utf-8'))\n",
    "    producer.flush()\n",
    "\n",
    "\n",
    "def spark_kafka():\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1 pyspark-shell'\n",
    "    # conf=SparkConf()\n",
    "    # conf.set(\"spark.executor.memory\", \"4g\")\n",
    "    # conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    raw_kafka_df = spark.readStream \\\n",
    "                        .format(\"kafka\") \\\n",
    "                        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "                        .option(\"subscribe\", 'plans') \\\n",
    "                        .option(\"startingOffsets\", \"earliest\") \\\n",
    "                        .load()\n",
    "\n",
    "    kafka_value_df = raw_kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "    output_query = kafka_value_df.writeStream \\\n",
    "                          .queryName(\"plans\") \\\n",
    "                          .format(\"memory\") \\\n",
    "                          .start()\n",
    "    output_query.awaitTermination(10)\n",
    "\n",
    "    value_df = spark.sql(\"select * from plans\")\n",
    "    value_df.show()\n",
    "    \n",
    "    value_rdd = value_df.rdd.map(lambda i: i['value'].split(\"\\t\"))\n",
    "    value_row_rdd = value_rdd.map(lambda i: Row(AttributesID=int(i[0]), \\\n",
    "                                                BeginPrimaryCareCostSharingAfterNumberOfVisits=int(i[1]), \\\n",
    "                                                bmi=float(i[2]), \\\n",
    "                                                children=int(i[3]), \\\n",
    "                                                smoker=i[4], \\\n",
    "                                                region=i[5], \\\n",
    "                                                charges=i[6]))\n",
    "\n",
    "    df = spark.createDataFrame(value_row_rdd)\n",
    "    df.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # df.printSchema()\n",
    "    \n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "         .mode('append') \\\n",
    "         .option('database','health_insurance') \\\n",
    "         .option('collection', 'health_insurance_plan') \\\n",
    "         .option('uri', \"mongodb://127.0.0.1/health_insurance.dbs\") \\\n",
    "         .save()\n",
    "\n",
    "def main():\n",
    "    kafka_prod()\n",
    "    spark_kafka()\n",
    "#     \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5. The Database Name in this code is called health_insurance and collection name is health_insurance_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get the data from the following url: \n",
    "# https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/Network.csv\n",
    "import requests, os\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "def kafka_prod():\n",
    "    producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "    response = requests.get(\"https://raw.githubusercontent.com/platformps/Healthcare-Insurance-Data/master/Network.csv\")\n",
    " \n",
    "    data_list = [data for data in response.text.splitlines()[1:]]\n",
    "    for data in data_list:\n",
    "        #print(data)\n",
    "        producer.send('network1', data.encode('utf-8'))\n",
    "    producer.flush()\n",
    "\n",
    "\n",
    "def spark_kafka():\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1 pyspark-shell'\n",
    "    # conf=SparkConf()\n",
    "    # conf.set(\"spark.executor.memory\", \"4g\")\n",
    "    # conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    spark = SparkSession.builder.config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/\").getOrCreate()\n",
    "    \n",
    "    raw_kafka_df = spark.readStream \\\n",
    "                        .format(\"kafka\") \\\n",
    "                        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "                        .option(\"subscribe\", 'network1') \\\n",
    "                        .option(\"startingOffsets\", \"earliest\") \\\n",
    "                        .load()\n",
    "\n",
    "    kafka_value_df = raw_kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "    output_query = kafka_value_df.writeStream \\\n",
    "                          .queryName(\"network1\") \\\n",
    "                          .format(\"memory\") \\\n",
    "                          .start()\n",
    "    output_query.awaitTermination(10)\n",
    "\n",
    "    value_df = spark.sql(\"select * from network1\")\n",
    "    value_df.show()\n",
    "    \n",
    "    value_rdd = value_df.rdd.map(lambda i: i[\"value\"].split(\",\"))\n",
    "#     value_rdd.foreach(lambda i: print(len(i)))\n",
    "    value_row_rdd = value_rdd.map(lambda i: Row(BusinessYear=i[0], \\\n",
    "                                                StateCode=i[1], \\\n",
    "                                                IssuerId=int(i[2]), \\\n",
    "                                                SourceName=i[3], \\\n",
    "                                                VersionNum=int(i[4]), \\\n",
    "                                                ImportDate=i[5], \\\n",
    "                                                IssuerId2=int(i[6]), \\\n",
    "                                                StateCode2=i[7], \\\n",
    "                                                NetworkName=i[8], \\\n",
    "                                                NetworkId=i[9], \\\n",
    "                                                NetworkURL=i[10], \\\n",
    "                                                RowNumber=i[11], \\\n",
    "                                                MarketCoverage=i[12], \\\n",
    "                                                DentalOnlyPlan=i[13]))\n",
    "\n",
    "    df = spark.createDataFrame(value_row_rdd)\n",
    "    df.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # df.printSchema()\n",
    "    \n",
    "#     df.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "#         .mode('append') \\\n",
    "#         .option('database','cdw_sapp') \\\n",
    "#         .option('collection', 'cdw_sapp_service') \\\n",
    "#         .option('uri', \"mongodb://127.0.0.1/cdw_sapp.dbs\") \\\n",
    "#         .save()\n",
    "\n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .mode('append') \\\n",
    "        .option('database','health_insurance') \\\n",
    "        .option('collection', 'health_insurance_network') \\\n",
    "        .save()\n",
    "\n",
    "def main():\n",
    "    # kafka_prod()\n",
    "    spark_kafka()\n",
    "#     \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Visualization Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A)​ ​Use “Service Area Dataset” from MongoDB. Find and plot the count of ​ServiceAreaName, SourceName , and BusinessYear ​across the country each state? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyMongo to Connect the MongoClient and Find the ServiceAreaName, SourceName , and BusinessYear \n",
    "# across the country each state\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "client = MongoClient('localhost',27017)\n",
    "# print('Connect to the MongoDB Client')\n",
    "db = client.health_insurance\n",
    "collection = db.health_insurance_service\n",
    "data = pd.DataFrame(list(collection.find()))\n",
    "state_count_df = data.groupby('StateCode').count()\n",
    "a = state_count_df[['ServiceAreaName','SourceName','BusinessYear']]\n",
    "a.plot.bar(figsize=(10,10))\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(B)​ ​Use “Service Area Dataset” from MongoDB. Find and plot the count of “​sources​” across the country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following plot shows the sourcename for the country.\n",
    "source_count_df = data.groupby('SourceName').count()\n",
    "b = state_count_df[['SourceName','County']]\n",
    "b.plot.bar(figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C)​ ​Use the “Benefit Cost Sharing” dataset from MongoDB. Find and plot the number of benefit plans in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyMongo to Connect the MongoClient and using Benefit Cost Sharing dataset.\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "client = MongoClient('localhost',27017)\n",
    "# print('Connect to the MongoDB Client')\n",
    "d1 = db.health_insurance_benefit\n",
    "d2 = pd.DataFrame(list(d1.find()))\n",
    "d3 = d2[['BenefitName','StateCode']].groupby(['StateCode'])['BenefitName'].count()\n",
    "d3.plot.bar(figsize = (10,10))\n",
    "plt.show()\n",
    "# The plot will show the benefit plans in each state, Wisconsin has the highest benefit plans in the country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(D) ​ ​Use the “Insurance” dataset from MongoDB and find the number of mothers who smoke and also have children. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pymongo module and using insurance dataset\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "client = MongoClient('localhost',27017)\n",
    "# print('Connect to the MongoDB Client')\n",
    "e1 = db.health_insurance_insurancecharges\n",
    "e2 = pd.DataFrame(list(e1.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3=('The number of mother who smoke and also have children are ', e2[(e2['sex']=='female') & \\\n",
    "                                                                 (e2['smoker']=='yes') & \\\n",
    "                                                                 (e2['children']>0)]['_id'].count())\n",
    "print(e3)\n",
    "# The number of monther who smoke and also have children are 124 according to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(E) ​Use the “Insurance” dataset from MongoDB. Find out which region has the highest rate of smokers. Plot the results for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyMongo to Connect the MongoClient and using insurance dataset to find out the region has the highest rate of smokers.\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "client = MongoClient('localhost',27017)\n",
    "print('Connect to the MongoDB Client')\n",
    "f1 = db.health_insurance_insurancecharges\n",
    "f2 = pd.DataFrame(list(f1.find()))\n",
    "f3 = f2.groupby('region').count()['_id']\n",
    "sm = f2[f2.smoker == 'yes'].groupby('region').count()['_id']\n",
    "xyz = sm / f3\n",
    "xyz.plot.bar(figsize=(10,10))\n",
    "plt.show()\n",
    "# The following graph shows that Southeastern region has the hightest rate more smokers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. User Menu"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part 1. Get the user input for the menu"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part 2. Choose the Options\n",
    "Option 1: To see Credit Card data, Health Insurance Data and Visualization.\n",
    "Option 2: Health Insurance Marketplace Data Files includes BenefitsCostSharing, Insurance PlanAttributes, Network, and ServiceArea.\n",
    "Option 3: Visualization should contain: \n",
    "(A) Use “Service Area Dataset” from MongoDB. Find and plot the count of ServiceAreaName, SourceName , and BusinessYear ​across the country each state? \n",
    "(B) Use “Service Area Dataset” from MongoDB. Find and plot the count of “ sources” across the country.\n",
    "(D) Use the “Benefit Cost Sharing” dataset from MongoDB. Find and plot the number of benefit plans in each state. \n",
    "(E) Use the “Insurance” dataset from MongoDB and find the number of mothers who smoke and also have children\n",
    "(F)  Use the “Insurance” dataset from MongoDB. Find out which region has the highest rate of smokers. Plot the results for each region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import branch\n",
    "import creditcard\n",
    "import customer\n",
    "import Benefit1\n",
    "import Benefit2\n",
    "import Benefit3\n",
    "import Benefit4\n",
    "import Insurance\n",
    "import PlansAttribute\n",
    "import Network\n",
    "import ServiceArea\n",
    "import VA\n",
    "import VB\n",
    "import VD\n",
    "import VE\n",
    "import VF\n",
    "\n",
    "def main(): \n",
    "    print('Hello!')\n",
    "    entry = None\n",
    "    while entry != '0':\n",
    "        entry = input('\\n1) If you would like to see credit card data please press 1\\\n",
    "                    \\n2) If you would like to see Health Insurance data please press 2\\\n",
    "                    \\n3) If you would like to visualize or analyze uploaded data please press 3\\\n",
    "                    \\n4) Otherwise please press 0, and you will be logged out from the system\\\n",
    "                    \\n\\nPlease choose one of the following options 1, 2, 3, or 4:\\\n",
    "                    \\n-------- ')\n",
    "        if entry =='1':\n",
    "            case = input('\\n1) If you would like to see branch data please press 1\\\n",
    "                    \\n2) If you would like to see credit card data please press 2\\\n",
    "                    \\n3) If you would like to see customers data please press 3\\\n",
    "                    \\n4) Otherwise please press 0, and you will return to the previous menu\\\n",
    "                    \\n-------- ')\n",
    "            if case =='1':\n",
    "                from Case_Study import branch \n",
    "            elif case=='2':\n",
    "                from Case_Study import creditcard\n",
    "            elif case=='3':\n",
    "                from Case_Study import customer\n",
    "            else:\n",
    "                print('You are returned to main menu')\n",
    "        elif entry =='2':\n",
    "            case = input('\\nHealth Insurance Marketplace Data Files:\\\n",
    "                           \\n\\t1) BenefitsCostSharing\\\n",
    "                           \\n\\t2) Insurance\\\n",
    "                           \\n\\t3) PlanAttributes\\\n",
    "                           \\n\\t4) Network\\\n",
    "                           \\n\\t5) ServiceArea\\\n",
    "                           \\n\\t6) Otherwise please press g, and you will return to the previous menu\\\n",
    "                           \\n\\nPlease select anything from the list\\\n",
    "                           \\n----------- ')\n",
    "            if case == '1':\n",
    "                from Case_Study import Benefit1, Benefit2, Benefit3, Benefit4\n",
    "            elif case =='2':\n",
    "                from Case_Study import Insurance\n",
    "            elif case =='3':\n",
    "                from Case_study import PlansAttribute   \n",
    "            elif case =='4':\n",
    "                from Case_Study import Network            \n",
    "            elif case =='5':\n",
    "                from Case_Study import ServiceArea\n",
    "            else:\n",
    "                print('You are returned to main menu')            \n",
    "        elif entry =='3':\n",
    "            case = input('nChoose on of the following options:\\\n",
    "            \\n\\t1) Here you can see counts of ServiceAreaName, SourceName, and BusinessYear by state\\\n",
    "                           \\n\\t2) Here you can see the counts of sources across the country\\\n",
    "                           \\n\\t3) Invalid option\\\n",
    "                           \\n\\t4) Here you can see the number of benefit plans in each state\\\n",
    "                           \\n\\t5) Here you can see quantity of smoking mother\\\n",
    "                           \\n\\t6) Here you can see highest smoking region\\\n",
    "                           \\n\\t7) Otherwise please press 0, and you will return to the previous menu\\\n",
    "                           \\n\\nPlease select an option from the list\\\n",
    "                           \\n------- ')\n",
    "            if case=='1':\n",
    "                from Case_Study import VA\n",
    "            elif case =='2':\n",
    "                from Case_Study import VB\n",
    "            elif case =='4':\n",
    "                from Case_Study import VD         \n",
    "            elif case =='5':\n",
    "                from Case_Study import VE            \n",
    "            elif case =='6':\n",
    "                from Case_Studt import VF\n",
    "            else:\n",
    "                print('Main Menu')\n",
    "    print('Closed')\n",
    "                    \n",
    "           \n",
    "if __name__=='__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(1) In this case study, we have learned how to use Python to connect the MariaDB. Using MariaDB to connect and learn how to program in Spark SQL and connect to MongoDB and named the database called cdw_sapp and collection name called cdw_sapp_branch, cdw_sapp_creditcard and cdw_sapp_customer.\n",
    "(2) Extracting JSON, CSV and other types of data from web URLs using the Spark Streaming integration for Kafka, a stream-processing software. \n",
    "(3) Using the Kafka connector in the Spark Structured Streaming in order to manage the transformation of the data before loading into MongoDB collections and send it to MongoDB. \n",
    "(4) Finally, Use Data Visuaization to analyze the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
